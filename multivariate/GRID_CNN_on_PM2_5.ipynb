{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of GRID_LSTM_on_PM2_5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gagandt/modelling-air-pollution/blob/master/multivariate/GRID_CNN_on_PM2_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsUPIuOTe2J4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "82255820-1497-4e14-f7c4-8fd52bf08caa"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccuaWFsXWYMK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pandas import DataFrame\n",
        "import pandas\n",
        "from pandas import concat\n",
        "\n",
        "# convert series to supervised learning\n",
        "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
        "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
        "\tdf = DataFrame(data)\n",
        "\tcols, names = list(), list()\n",
        "\t# input sequence (t-n, ... t-1)\n",
        "\tfor i in range(n_in, 0, -1):\n",
        "\t\tcols.append(df.shift(i))\n",
        "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "\t# forecast sequence (t, t+1, ... t+n)\n",
        "\tfor i in range(0, n_out):\n",
        "\t\tcols.append(df.shift(-i))\n",
        "\t\tif i == 0:\n",
        "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
        "\t\telse:\n",
        "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "\t# put it all together\n",
        "\tagg = concat(cols, axis=1)\n",
        "\tagg.columns = names\n",
        "\t# drop rows with NaN values\n",
        "\tif dropnan:\n",
        "\t\tagg.dropna(inplace=True)\n",
        "\treturn agg"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOcDLW3bjwGl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "75f7937f-42e5-4e3c-e17e-d9e49cb0f9f4"
      },
      "source": [
        "import os\n",
        "from pandas import read_csv\n",
        "from datetime import datetime\n",
        "# load data\n",
        "os.chdir('/content/drive/My Drive/Colab Notebooks')\n",
        "def parse(x):\n",
        "\treturn datetime.strptime(x, '%Y %m %d %H')\n",
        "dataset = read_csv('./data/beijing.csv',  parse_dates = [['year', 'month', 'day', 'hour']], index_col=0, date_parser=parse)\n",
        "dataset.drop('No', axis=1, inplace=True)\n",
        "# manually specify column names\n",
        "dataset.columns = ['pollution', 'dew', 'temp', 'press', 'wnd_dir', 'wnd_spd', 'snow', 'rain']\n",
        "dataset.index.name = 'date'\n",
        "# mark all NA values with 0\n",
        "dataset['pollution'].fillna(0, inplace=True)\n",
        "# drop the first 24 hours\n",
        "dataset = dataset[24:]\n",
        "# summarize first 5 rows\n",
        "print(dataset.head(5))\n",
        "# save to file\n",
        "dataset.to_csv('pollution.csv')\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                     pollution  dew  temp   press wnd_dir  wnd_spd  snow  rain\n",
            "date                                                                          \n",
            "2010-01-02 00:00:00      129.0  -16  -4.0  1020.0      SE     1.79     0     0\n",
            "2010-01-02 01:00:00      148.0  -15  -4.0  1020.0      SE     2.68     0     0\n",
            "2010-01-02 02:00:00      159.0  -11  -5.0  1021.0      SE     3.57     0     0\n",
            "2010-01-02 03:00:00      181.0   -7  -5.0  1022.0      SE     5.36     1     0\n",
            "2010-01-02 04:00:00      138.0   -7  -5.0  1022.0      SE     6.25     2     0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmrHMLa5MwK0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy\n",
        "import os\n",
        "from math import sqrt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "def grid_search(dataset, lookback, filters, kernels, batch_sizes, epochs, split_percentage):\n",
        "#   os.chdir('MLP')\n",
        "  scores = list()\n",
        "  \n",
        "  for lookback_period in lookback:\n",
        "    values = series_to_supervised(dataset, lookback_period, 1).values\n",
        "    \n",
        "    n_train_hours = (int)(split_percentage * len(values))\n",
        "    train = values[:n_train_hours, :]\n",
        "    test = values[n_train_hours:, :]\n",
        "    \n",
        "    train_X, train_y = train[:, :-1], train[:, -1]\n",
        "    test_X, test_y = test[:, :-1], test[:, -1]\n",
        "    \n",
        "    n_features = 8\n",
        "    n_hours = lookback_period\n",
        "    n_obs = n_hours * n_features\n",
        "    train_X, train_y = train[:, :n_obs], train[:, -n_features]\n",
        "    test_X, test_y = test[:, :n_obs], test[:, -n_features]\n",
        "\n",
        "    # reshape input to be 3D [samples, timesteps, features]\n",
        "    train_X = train_X.reshape((train_X.shape[0], n_hours, n_features))\n",
        "    test_X = test_X.reshape((test_X.shape[0], n_hours, n_features))\n",
        "    print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
        "    \n",
        "    for number_of_filters in filters:      \n",
        "        for kernel_size in kernels:\n",
        "          model = Sequential()\n",
        "          try:\n",
        "            model.add(Conv1D(filters=number_of_filters, kernel_size=kernel_size, activation='relu', input_shape=(n_hours, n_features)))\n",
        "            model.add(MaxPooling1D(pool_size=kernel_size))\n",
        "            model.add(Flatten())\n",
        "\n",
        "            model.add(Dense(1))\n",
        "            model.compile(loss='mae', optimizer='adam')\n",
        "\n",
        "            ref_model = copy.deepcopy(model)\n",
        "\n",
        "            for batch_size in batch_sizes:\n",
        "              for number_of_epochs in epochs:         \n",
        "                model = copy.deepcopy(ref_model)\n",
        "\n",
        "                name = str(lookback_period)+'_'+str(number_of_filters)+'_'+str(kernel_size)+'_'+str(batch_size)+'_'+str(number_of_epochs)\n",
        "                print(\"Starting training for : \", name)\n",
        "\n",
        "                history = model.fit(train_X, train_y, epochs=number_of_epochs, batch_size=batch_size, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
        "                yhat = model.predict(test_X)\n",
        "\n",
        "                np.save('CNN_runtime/history' + name, history)\n",
        "                np.save('CNN_runtime/test_y' + name, test_y)\n",
        "                np.save('CNN_runtime/yhat' + name, yhat)\n",
        "\n",
        "                # print(yhat)\n",
        "                # print(test_X)\n",
        "                # print(test_y)\n",
        "                rmse = mean_squared_error(yhat, test_y)\n",
        "                print(\"RMSE for \", name, \" = \" , rmse)\n",
        "\n",
        "                scores.append([rmse, lookback_period, number_of_filters, kernel_size, batch_size, number_of_epochs])\n",
        "          except:\n",
        "            print('error')\n",
        "\n",
        "  \n",
        "#   os.chdir('..')\n",
        "  return scores\n",
        "          \n",
        "  "
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ei1UVrcEkbrD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# load dataset\n",
        "dataset = read_csv('pollution.csv', header=0, index_col=0)\n",
        "values = dataset.values\n",
        "\n",
        "# integer encode direction\n",
        "encoder = LabelEncoder()\n",
        "values[:,4] = encoder.fit_transform(values[:,4])\n",
        "\n",
        "# ensure all data is float\n",
        "values = values.astype('float32')\n",
        "\n",
        "# normalize features\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled = scaler.fit_transform(values)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLtlf_6lbPuI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "58cedf0b-a15b-435c-8a18-72581fe7f0c8"
      },
      "source": [
        "lookback = [3,5,7]\n",
        "filters = [2,4,16, 32, 64]\n",
        "kernels = [2,3, 5]\n",
        "# batch_size = [50]\n",
        "batch_size = [50, 75, 100]\n",
        "# epochs = [1]\n",
        "epochs = [50,100]\n",
        "split_percentage = 0.8\n",
        "\n",
        "scores = grid_search(values, lookback, filters, kernels, batch_size, epochs, split_percentage)\n",
        "print(scores)\n",
        "np.save('CNN_scores', np.array(scores))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(35037, 3, 8) (35037,) (8760, 3, 8) (8760,)\n",
            "Starting training for :  3_64_2_50_1\n",
            "Train on 35037 samples, validate on 8760 samples\n",
            "Epoch 1/1\n",
            " - 1s - loss: 24.4942 - val_loss: 20.3787\n",
            "RMSE for  3_64_2_50_1  =  1068.9691\n",
            "error\n",
            "Starting training for :  3_4_2_50_1\n",
            "Train on 35037 samples, validate on 8760 samples\n",
            "Epoch 1/1\n",
            " - 1s - loss: 31.9270 - val_loss: 21.5732\n",
            "RMSE for  3_4_2_50_1  =  1343.2654\n",
            "error\n",
            "Starting training for :  3_16_2_50_1\n",
            "Train on 35037 samples, validate on 8760 samples\n",
            "Epoch 1/1\n",
            " - 1s - loss: 44.8895 - val_loss: 18.3046\n",
            "RMSE for  3_16_2_50_1  =  1034.0762\n",
            "error\n",
            "(35036, 5, 8) (35036,) (8759, 5, 8) (8759,)\n",
            "Starting training for :  5_64_2_50_1\n",
            "Train on 35036 samples, validate on 8759 samples\n",
            "Epoch 1/1\n",
            " - 1s - loss: 28.1460 - val_loss: 20.1438\n",
            "RMSE for  5_64_2_50_1  =  1124.1605\n",
            "Starting training for :  5_64_3_50_1\n",
            "error\n",
            "Starting training for :  5_4_2_50_1\n",
            "Train on 35036 samples, validate on 8759 samples\n",
            "Epoch 1/1\n",
            " - 1s - loss: 191.7950 - val_loss: 95.9669\n",
            "RMSE for  5_4_2_50_1  =  17962.05\n",
            "Starting training for :  5_4_3_50_1\n",
            "Train on 35036 samples, validate on 8759 samples\n",
            "Epoch 1/1\n",
            " - 1s - loss: 40.2613 - val_loss: 23.0232\n",
            "RMSE for  5_4_3_50_1  =  1521.3041\n",
            "Starting training for :  5_16_2_50_1\n",
            "Train on 35036 samples, validate on 8759 samples\n",
            "Epoch 1/1\n",
            " - 1s - loss: 33.6820 - val_loss: 28.3609\n",
            "RMSE for  5_16_2_50_1  =  1813.6475\n",
            "Starting training for :  5_16_3_50_1\n",
            "Train on 35036 samples, validate on 8759 samples\n",
            "Epoch 1/1\n",
            " - 1s - loss: 30.5300 - val_loss: 24.1501\n",
            "RMSE for  5_16_3_50_1  =  1588.4418\n",
            "(35034, 7, 8) (35034,) (8759, 7, 8) (8759,)\n",
            "Starting training for :  7_64_2_50_1\n",
            "Train on 35034 samples, validate on 8759 samples\n",
            "Epoch 1/1\n",
            " - 1s - loss: 26.9702 - val_loss: 16.8185\n",
            "RMSE for  7_64_2_50_1  =  893.9211\n",
            "Starting training for :  7_64_3_50_1\n",
            "Train on 35034 samples, validate on 8759 samples\n",
            "Epoch 1/1\n",
            " - 1s - loss: 37.5616 - val_loss: 30.7986\n",
            "RMSE for  7_64_3_50_1  =  2406.0193\n",
            "Starting training for :  7_4_2_50_1\n",
            "Train on 35034 samples, validate on 8759 samples\n",
            "Epoch 1/1\n",
            " - 1s - loss: 196.2168 - val_loss: 38.9410\n",
            "RMSE for  7_4_2_50_1  =  2836.4595\n",
            "Starting training for :  7_4_3_50_1\n",
            "Train on 35034 samples, validate on 8759 samples\n",
            "Epoch 1/1\n",
            " - 1s - loss: 49.5466 - val_loss: 47.5213\n",
            "RMSE for  7_4_3_50_1  =  3846.23\n",
            "Starting training for :  7_16_2_50_1\n",
            "Train on 35034 samples, validate on 8759 samples\n",
            "Epoch 1/1\n",
            " - 1s - loss: 39.0230 - val_loss: 17.7941\n",
            "RMSE for  7_16_2_50_1  =  978.3474\n",
            "Starting training for :  7_16_3_50_1\n",
            "Train on 35034 samples, validate on 8759 samples\n",
            "Epoch 1/1\n",
            " - 1s - loss: 38.8922 - val_loss: 32.1834\n",
            "RMSE for  7_16_3_50_1  =  2606.4365\n",
            "[[1068.9691, 3, 64, 2, 50, 1], [1343.2654, 3, 4, 2, 50, 1], [1034.0762, 3, 16, 2, 50, 1], [1124.1605, 5, 64, 2, 50, 1], [17962.05, 5, 4, 2, 50, 1], [1521.3041, 5, 4, 3, 50, 1], [1813.6475, 5, 16, 2, 50, 1], [1588.4418, 5, 16, 3, 50, 1], [893.9211, 7, 64, 2, 50, 1], [2406.0193, 7, 64, 3, 50, 1], [2836.4595, 7, 4, 2, 50, 1], [3846.23, 7, 4, 3, 50, 1], [978.3474, 7, 16, 2, 50, 1], [2606.4365, 7, 16, 3, 50, 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}